# 1. 비정형 데이터 
: 연산 불가능
: 동영상, 이미지, 음성, 문서, 메일 등
: 구조(Structure)가 아님

# 2. 상관 계수
1. 피어스만 상관계수 
   : 두 변수 간의 선형 관계를 측정하는 통계량 
   : -1 ~ 1 사이의 값을 가진다
   : 1에 가까울수록 강한 양의 선형 관계, 즉, 한 변수가 증가하면 다른 변수도 증가
   : -1에 가까울수록 강한 음의 선형관계.
   : 0에 가까울 수록, 두 변수 간에 선형적인 관계가 전혀 없음 
2. 스피어만 상관계수
   : 변수들이 서열 척도로 측정될 때, 그들 간의 순위 상관관계를 측정하는 비모수적 방법
3. Phi 계수 
   : 두 이진 변수의 상관관계를 측정하는 통계량

# 3. PCA(주성분 분석)
: 정규분포를 가정하지 X
: 데이터를 설명하는 데 중요한 방향을 찾기 위해 분산이 큰 방향을 확인

# 4. **표본분산**
: 표본분산은 모분산과 달리 전체 표본수를 n이 아닌 **n-1**로 둔다 

# 5. 제 1종 오류, 제 2종 오류
1. **제 1종 오류**
   : 귀무가설이 참일 때, 이를 기각하는 오류
   : **실제로는 차이가 없는데, 데이터 분석 결과 효과나 차이가 있다고 잘못 결론짓는 오류**
2. **제 2종 오류**
   : 대립가설이 참일 때, 귀무가설을 기각하지 못하는 오류 
   : **실제로는 효과나 차이가 있는 데, 데이터 분석 결과에서 효과나 차이가 없다고 결론 짓는 오류** 

# 6.  Z-score, Binning 
1. **Z-score**
   : **데이터 평균 0, 표준편차를 1로 변환**
2. Binning
   : 데이터를 여러 개의 구간(bin)으로 나누어 각 구간에 속하는 값을 하나의 대표값으로 변환하는 방법 

# 7. 다중공선성
: 독립변수들 간에 높은 상관성을 존재하는 문제. 클수록 회분석에서 문제
: 분산 팽창지수(VIF) : 다중공선성을 평가하는 지표

# 8. SVM
: 선형과 비선형 분류에 이용, 일반화 성능이 높고, 신규 데이터에도 잘 동작함.
: 계산량이 많아서 학습속도는 느림
: 초매개변수 튜닝 등 최적화가 중요함

# 9. 비모수검정
: 데이터의 분포에 대한 가정 없이, 데이터의 순위나 순서에 따라 가설을 검정하는 방법
: 정규성을 가정하기 어렵거나 표본 집단의 크기가 작은 경우
: 검정력이 떨어짐

# 9-1 모수 검정
: 특정한 확률 분포의 모수에 대한 **가설을 검정하는 방법**
: 확률분포 검정

# 9-2 F-검정
: 2개 이상의 집단 간의 **분산**을 비교하여, 그들이 동일한 분산을 가지고 있는지 평가

# 9-3 t-검정
: 두 그룹 간의 **평균**을 비교하여, 두 그룹 간의 차이가 통계적으로 유의미한지 평가

# 9-4 z-검정
: **모집단의 평균**에 대한 가설 검정을 수행하는 통계적 방법 

# 10. 규제
1. Lasso :  L1 규제 : 가중치 절대값을 규제 항으로 추가
2. Ridge : L2 규제 : 가중치 제곱합에 비례하는 패널티 
3. Logistic Regression(로지스틱 회귀모형) : L1과 L2의 결합: 독립 변수들로부터 두 범주만을 가지는 종속 변수를 예측하는데 사용 

# 11. 회귀대치법
: 결측값을 예측하여 대치하기 위해 회귀분석을 이용
: 결측값이 없는 변수(독립변수)를 이용하여 결측값이 있는 변수(종속변수)를 예측

# 12. 척도 
1. 비율 척도 : 숫자와 그 간격의 의미
2. 명목 척도 : 분류만을 위한 척도

# 13. 분석 기획 우선순위 고려요소 
1. 전략적 중요도, 실행 용이성 관점
2. 투자비용 요소, 비즈니스 효과 요소
3. 분석 ROI요소 (시급성과 난이도)

# 14. 기업 분석 수준 진단
: 분석 준비도를 이용하여 진단 가능
: 조직 내 데이터 분석 업무 도입을 목적으로 현재 수준을 파악하기 위한 진단 방법
1. 분석 업무
2. 분석 인력 및 조직 
3. 분석 기법
4. 분석 데이터
5. 분석 문화
6. 분석 인프라 

# 15. Redis, CouchDB, DynamoDB
1. Redis 
   : 메모리 기반의 <키,값> 저장 공간 사용
   : 메모리에 저장된 내용을 지속시키기 위해 파일로 싱크하는 기능 제공
2. **CouchDB**
   : **문서 단위의 ACID속성 지원**
   : 데이터가 여러 시점에서 접근할 때, 발생할 수 있는 문제점을 다중 버전 동시 동작 제어 기능으로 해결 
3. **DynamoDB**
   : **응용 프로그램에 따른 DB 자동 분할** 
# 16. 명목형 데이터
: 혈액형, 학력, 성별과 같은 카테고리나 라벨을 나타내는 데이터.
: 순서나 크기를 비교할 수 없음 

# 17. 병렬화
1. 배깅 
   : 여러 개의 개별 모델을 조합하여, 최적의 모델로 일반화시키는 앙상블
2. 부스팅 
   : 약한 학습기 -> 강한 학습기
3. 랜덤 포레스트
   : 배깅과 동일하되, 트리들의 상관성을 제거하여 보다 안정적인 성능제공

# 18. 군집
1. **가우시안 혼합행렬**
   : **각 분포에 속할 확률이 높은 데이터로 군집화하는 기법**
2. 스펙트럼 군집 분석
   : 그래프 기반 군집화 기법, 데이터의 상대적 관계/유사성을 고려해서 군집 개수를 설정 
3. **k-평균 군집분석**
   : **군집 수를 지정하고 각 개체를 가까운 초기값에 할당하여 군집을 형성한 뒤, 각 군집의 평균을 재계산**
   : 엘보우(elbow 기법)
	   : 군집 간 분산과 전체 분산 간의 비율을 k값에 따라 구하며, 완곡하게 줄어든 부분에서 k 값을 선택한다. 

# 19. k-폴드 교차검증
: 학습 데이터 수가 아닌 분할된 폴드의 수만큼 학습과 검증을 반복 수행
: 매번 학습마다 **k-1개의 학습데이터**로 나머지 1개를 검증 데이터로 사용
: **k가 클수록 학습효과가 큼**
: 학습속도는 느림

# 20. 이산확률 분포, 연속 확률 분포
1. 이산확률분포 : 다항, 포아송, 기하
2. 연속확률분포 : 지수, 분포 

# 21. LSTM, GRU 
1. LSTM(Long short term memory)
   : 입력, 출력, 삭제 게이트 사용
2. GRU(Gated Recurrent Unit)
   : 입력, 출력, 삭제 게이트 대신에 업데이트, 리셋 게이트로 효율성 높임
   : LSTM의 보완 모델 

# 22. 공분산
: 동일 시간대에서 두 변수 간 상관관계 분석 
: 공분산이 =0  : 두 변수 간의 선형적인 관계가 없음, 완전히 독립적이라는 것은 아님. 다른 종류의 상관성은 존재할 수 있음. 

# 23. 지수평할법
: 최근 자료에 더 높은 가중치를 부여

# 24. **Leave one out** 교차 검증
: SVM의 하이퍼파라미터 최적화 과정에서 두 명의 분석가의 분석결과를 동일하게 하기 위한 방법으로 적절
: 각 데이터 포인트를 테스트셋으로 사용. 나머지 데이터를 훈련셋으로 사용해서 모델을 평가

# 25. DBSCAN 
: 군집화 알고리즘 중에서 군집의 수를 지정하지 않아도 되는 것
: **데이터 밀도를 기반**으로 클러스터를 형성하는 밀도기반 군집화 알고리즘
: 밀도가 높은 지역 : 클러스터로 인식하고, 데이터포인터들이 서로 밀집하게 연결되어 있는 밀집 지역을 찾는 방식으로 작동 

# 26. FGI(Focus group interview)
: 관찰자 역할의 연구자가 6-12명 정도의 동일한 소수 집단을 대상으로 특정 주제에 대하여 자유로운 토론을 이끌어내 자료를 수집한다. 

# 27. 빅데이터 플랫폼 구조
1. 소프트웨어 계층 
   : 데이터 수집과 처리 및 분석을 하는 응용소프트웨어가 처리되는 영역
2. 플랫폼 계층
   : 작업 관리나 데이터 및 자원 할당과 관리 등이 이루어지는 영역
3. 인프라 스트럭처 계층
   : 네트워크나 스토리지 등 자원 제공 및 관리를 수행하는 영역 

# 28. 분포 
1. 포아송 분포
   : 단위 시간 안에서 어떤 사건이 몇 번 발생하는지 나타내는 이산확률 분포
2. 기하 분포
   : 첫번째 성공까지 시행횟수를 모델링
3. **정규 분포**
   : **연속 확률 변수의 분포를 모델링**
4. 이항 분포
   : 동일한 확률로 이진 결과를 갖는 여러 독립적인 시행을 모델링한다. 

# 29. 불균형 데이터 처리
1. 언더 샘플링 
   : 다수 클래스의 샘플을 제거
2. 오버 샘플링
   : 소수 클래스의 샘플을 복제하거나 새로운 샘플을 추가
3. 가중치 균형방법
   : 모델 학습 시 소수 클래스에 높은 가중치 부여
4. 비용 민감 학습 (cost sensitive learning)
   : 소수 클래스에 더 많은 가중치를 부여 
   
: 불균형 데이터는 다수의 클래스의 영향으로 정확도가 높게 나타나지만 소수 클래스의 재현율은 급격히 작아지는 문제가 있다. 


# 30. 왜도 값에 따른 변환 방법
1. + 
   : 로그변환, 제곱근 변환, 역수 변환
2. -
   : 지수변환, 제곱변환 

# 31. 타깃 인코딩
: 종속 변수를 활용하여 범주형 특성을 인코딩하는 기법
: 주로 분류 문제에서 사용
: 각 범주에 대한 종속 변수의 평균 값을 인코딩으로 사용

# 32. 변수 척도
1. 연속형 척도
   : 평균, 표준편차와 같은 기술량으로 구할 수 있음
2. 범주형 척도
   : 상대빈도, 분포표, 비율 등을 사용하여 설명하는 것이 일반적 

# 33. 드롭아웃 효과
: 신경망에서 과적합을 방지하기 위해 사용되는 정규화 기법
: 학습 중 **일부 뉴런을 무작위 제거**하여 일반화 능력을 향상 시킴

# 34. 데이터 증강
: 기존 데이터에 노이즈를 추가하는 등 다양한 변형을 가함으로써 신경모델의 과적합을 방지할 수 있음. 

# 35. 텍스트 마이닝 기법
1. TF-IDF(Term frequency inverse Document frequency)
   : 단어의 빈도에 역문서빈도(IDF)를 곱하여 각 단어들마다 가중치를 부여해서 중요도를 높이는 방법 
2. Pos-tagging 
   : 텍스트에서 단어의 품사를 식별하고, 태깅 붙이는 절차
   (전처리 시 사용)
3. 원핫 인코딩 
   : 단어를 벡터로 인코딩, 표현하고자 하는 단어를 1, 나머지는 0
4. **Bag of words** 
   : 단어를 키, 빈도를 벨류, 단어 **빈도만 계산**하여 변환 

# 36. 신뢰도
: A를 포함한 거래 중 항목 A와 B를 같이 포함할 확률
: A를 구매했을 때, B도 구매할 것 =>  신뢰도가 높다. 

# 37. 머신러닝 기반 데이터 분석 결과를 공유 또는 유지보수를 위해 관리하는 산출물 
- 결과 산출물
1. 분석 계획서
2. 데이터 확보 방안
3. 분석결과 및 예측 결과
4. 비즈니스 성과
5. 사용 및 유지보수 가이드

# 38. 정규성 검정 기법
1. q-q 플롯
   : 주어진 데이터와 정규분포의 분위 수를 비교하여 정규성을 평가하는 그래픽 기법
2. 샤피로-월크 검정
   : 표본 크기가 작을 때 사용. 데이터 표본 통계량과 정규 분포의 기댓값 및 분산 사이의 차이를 평가
3. 콜모고로프 스미로노프 검정 
   : 데이터의 누적분포함수와 정규분포의 누적분포함수 사이의 차이를 평가

# 39. **카이제곱 검정**
: 범주형 변수들 간의 **상관관계** 검정
: **주어진 데이터가 특정 가설에 따라 기대되는 분포와 일치하는 지 확인** 

# 40. 과적합을 해결하기 위한 방법
1. 데이터 양을 늘려 더 많은 데이터 학습 
2. 데이터 표준화, 정규화 -> 이상치 제거
3. 유용하지 않거나 중복되는 피처 제거
4. 파라미터수 감소. L1, L2 규제
   : 가중치 줄이고 복잡성 줄임
5. drop-out
   : 신경망의 일부 뉴런을 선택하여 제외 
